<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description">
  <meta name="keywords" content="Nerfies, D-NeRF, NeRF">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>MindJourney: Test-Time Scaling with World Models</title>

  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-06E12DG85Q"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-06E12DG85Q');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/icon.ico"  type="image/x-icon">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
  <style>
    /* Optionally, ensure there is no clipping in publication-video containers */
    .publication-video {
      overflow: visible; /* or overflow: auto */
    }
    
    /* Enforce video scaling rules */
    .publication-video video {
      width: 100%;
      height: auto;
      display: block;
    }

    .title-logo {
      height: 1.3em;           /* equals cap-height of the text (Bulma is-1 ≈ 3 rem) */
      max-height: 1.3em;       /* safety */
      width: auto;              /* keep aspect ratio */
      max-width: 3.2em;         /* stops super-wide logo from overwhelming line */
      margin-right: 1.0rem;     /* breathe */
      vertical-align: middle;   /* baseline alignment */
    }

    /* 2. Gradient title in the same blue family as the logo */
    .gradient-text {
      background-image: linear-gradient(135deg, #0068bd 0%, #46a1f1 50%, #7fb2f3 100%);
      -webkit-background-clip: text;
      -webkit-text-fill-color: transparent;
      background-clip: text;   /* Firefox */
      color: transparent;
      font-weight: 700;
    }

    .author-break { display: block; }

    .video-section {
      padding-top:1.0rem;        /* tighter than Bulma default 3rem */
    }
    /* pull video section slightly up under hero */
    .hero + .video-section { margin-top:-0.75rem; }
  </style>
</head>
<body>

<!-- <nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://keunhong.com">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://hypernerf.github.io">
            HyperNeRF
          </a>
          <a class="navbar-item" href="https://nerfies.github.io">
            Nerfies
          </a>
          <a class="navbar-item" href="https://latentfusion.github.io">
            LatentFusion
          </a>
          <a class="navbar-item" href="https://photoshape.github.io">
            PhotoShape
          </a>
        </div>
      </div>
    </div>

  </div>
</nav> -->


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <!-- <h1 class="title is-1 publication-title">SnapMem: Snapshot-based 3D Scene Memory for Embodied Exploration and Reasoning</h1> -->
          <h1 class="title is-1 publication-title is-flex is-align-items-center is-justify-content-center">
              <img src="./static/images/mindjourney.png" alt="MindJourney Logo" class="title-logo" />
              <span class="gradient-text">MindJourney</span>
          </h1>
          <h2 class="subtitle is-3 publication-subtitle" style="font-size: 2.1rem">
            Test-Time Scaling with World Models for Spatial Reasoning      
          </h2>
          <!-- <h2 class="subtitle publication-subtitle" style="font-size: 1.7rem;color:#000000; margin-top: -7px;">
            CVPR 2025       -->
          <!-- </h2> -->
          <div class="is-size-5 publication-authors" style="margin-top: -7px;">
            <span class="author-block">
              <a href="https://yyuncong.github.io/">Yuncong Yang</a>*<sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://jiagengliu02.github.io/">Jiageng Liu</a>*<sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://cozheyuanzhangde.github.io/">Zheyuan Zhang</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://rainbow979.github.io/">Siyuan Zhou</a><sup>3</sup>,
            </span>
            <span class="author-block">
              <a href="https://cs-people.bu.edu/rxtan/">Reuben Tan</a><sup>4</sup>,
            </span>
            <br class="author-break"/>
            <span class="author-block">
              <a href="https://jwyang.github.io/">Jianwei Yang</a><sup>4</sup>,
            </span>
            <span class="author-block">
              <a href="https://yilundu.github.io/">Yilun Du</a><sup>5</sup>,
            </span>
            <span class="author-block">
              <a href="https://people.csail.mit.edu/ganchuang/">Chuang Gan</a><sup>1</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>UMass Amherst,</span>
            <span class="author-block"><sup>2</sup>JHU,</span>
            <span class="author-block"><sup>3</sup>HKUST,</span>
            <span class="author-block"><sup>4</sup>Microsoft Research,</span>
            <span class="author-block"><sup>5</sup>Harvard</span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block">(* indicates equal contribution)</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <!-- <span class="link-block">
                <a href="https://arxiv.org/pdf/2011.12948"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span> -->
              <span class="link-block">
                <a href="https://www.arxiv.org/abs/2507.12508"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Video Link. -->
              <!-- <span class="link-block">
                <a href="https://www.youtube.com/watch?v=PbnWizEJL8w"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span> -->
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/UMass-Embodied-AGI/MindJourney"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <span class="link-block">
                <a href="https://x.com/YuncongYY"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-twitter"></i>
                  </span>
                  <span>Twitter</span>
                  </a>
              </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- <section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video id="teaser" autoplay muted loop playsinline height="100%">
        <source src="./static/videos/teaser.mp4"
                type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        <span class="dnerf">Nerfies</span> turns selfie videos from your phone into
        free-viewpoint
        portraits.
      </h2>
    </div>
  </div>
</section> -->


<!-- <section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-steve">
          <video poster="" id="steve" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/steve.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-chair-tp">
          <video poster="" id="chair-tp" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/chair-tp.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-shiba">
          <video poster="" id="shiba" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/shiba.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-fullbody">
          <video poster="" id="fullbody" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/fullbody.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-blueshirt">
          <video poster="" id="blueshirt" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/blueshirt.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-mask">
          <video poster="" id="mask" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/mask.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-coffee">
          <video poster="" id="coffee" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/coffee.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-toby">
          <video poster="" id="toby" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/toby2.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section> -->


<section class="section">
  <div class="container is-four-fifths">

    <!-- Paper video. -->
    <div class="columns is-centered has-text-centered" style="margin-top: -10px;">
      <div class="column is-four-fifths">
        <!-- <h2 class="title is-3">Video</h2> -->
        <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths publication-video">
            <video src="static/videos/opening_video.mp4" controls muted playsinline></video>
          </div>
        </div>
      </div>
    </div>
    <!--/ Paper video. -->

    <!-- Abstract. -->
    <!-- <div class="columns is-centered has-text-centered">
      <div class="column" style="max-width: 60%; margin: 0 auto;">
        <h2 class="title is-3" style="padding-top: 80px;">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Constructing compact and informative 3D scene representations is essential for effective embodied exploration and reasoning, especially in complex environments over long periods. Existing scene representations, such as object-centric 3D scene graphs, have significant limitations. They oversimplify spatial relationships by modeling scenes as individual objects, with inter-object relationships described by restrictive texts, making it difficult to answer queries that require nuanced spatial understanding. Furthermore, these representations lack natural mechanisms for active exploration and memory management, which hampers their application to lifelong autonomy. In this work, we propose SnapMem, a novel snapshot-based scene representation serving as 3D scene memory for embodied agents. SnapMem employs informative images, termed Memory Snapshots, to capture rich visual information of explored regions. It also integrates frontier-based exploration by introducing Frontier Snapshots—glimpses of unexplored areas—that enable agents to make informed exploration decisions by considering both known and potential new information. Meanwhile, to support lifelong memory in active exploration settings, we further present an incremental construction pipeline for SnapMem, as well as an effective memory retrieval technique for memory management. Experimental results on three benchmarks demonstrate that SnapMem significantly enhances agents' exploration and reasoning capabilities in 3D environments over extended periods, highlighting its potential for advancing applications in embodied AI.           
          </p>
        </div>
      </div>
    </div> -->
    <!--/ Abstract. -->

    
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Spatial reasoning in 3D space is central to human cognition and indispensable for embodied tasks such as navigation and manipulation. However, state-of-the-art vision–language models (VLMs) struggle frequently with tasks as simple as anticipating how a scene will look after an egocentric motion: they perceive 2D images but lack an internal model of 3D dynamics. We therefore propose MindJourney, a test-time scaling framework that grants a VLM with this missing capability by coupling it to a controllable world model based on video diffusion. The VLM iteratively sketches a concise camera trajectory, while the world model synthesizes the corresponding view at each step. The VLM then reasons over this multi-view evidence gathered during the interactive exploration. Without any fine-tuning, our MindJourney achieves over an average 8% performance boost on the representative spatial reasoning benchmark SAT, showing that pairing VLMs with world models for test-time scaling offers a simple, plug-and-play route to robust 3D reasoning. Meanwhile, our method also improves upon the test-time inference VLMs trained through reinforcement learning, which demonstrates the potential of our method that utilizes world models for test-time scaling.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>

<!-- TODO: Change into our discriptions process-->
<section class="section">
  <div class="container is-max-desktop">
    <!-- Methodology. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">
        <!-- <h3 class="title is-4">How to empower VLM agents with lifelong exploration and reasoning abilities?</h3> -->
        <h3 class="title is-4" style="font-size: 1.40rem;">What if a VLM can actively explore in 3D Imagination Space before answering a Question?</h3>
        <h4 class="title is-5" style="padding-top: 20px; font-size: 1.3rem;">VLM Controls a World Model during Test-Time</h4>

        <video style="transform: scale(1.2); width: 80%; padding-top: 30px; padding-bottom: 40px;" autoplay muted loop playsinline preload="auto">
          <source src="static/videos/pipeline.mov" type="video/mp4">
          Your browser does not support the video tag.
        </video>

        <div class="content has-text-justified">
          <!-- <p>
            At each timestep, given a stream of egocentric views, <strong>SnapMem</strong> performs the following steps:
          </p> -->
          <ol style="text-align: left;">
            <li>At each step, the VLM guides the Spatial Beam Search in the 3D Imagination Space.</li>
            <li>During the Spatial Beam Search, the VLM stores helpful observations to the evidence buffer.</li>
            <li>VLM answers the question based on the evidence buffer.</li>
          </ol>
        </div>
        <h4 class="title is-5" style="padding-top: 30px; font-size: 1.3rem;">Trajectory Expansion with World Model</h4>

        <video style="width: 80%; padding-top: 0px; padding-bottom: 10px;" autoplay muted loop playsinline preload="auto">
          <source src="static/videos/traj_expansion.mov" type="video/mp4">
          Your browser does not support the video tag.
        </video>

        <div class="content has-text-justified">
          <ol style="text-align: left;">
            <li>At each step of the Spatial Beam Search, the World Model question-agnostically expands each beam nodes.</li>
            <li>The expanded observation set helps the VLM to choose the next beam node and proceed the Spatial Beam Search.</li>
            <li>The observation set is also the source of the evidence buffer.</li>
          </ol>
        </div>
      </div>
    </div>
    <!--/ Methodology. -->
  </div>
</section>


<!-- TODO: Change into our demo/examples -->
<section class="section">
  <div class="container is-max-desktop">

    <!-- Demo -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Examples</h2>
        <div class="columns is-centered has-text-centered">
          <div class="column is-full-width">
            <div class="publication-video" style="display: flex; flex-direction: column; align-items: center; margin-bottom: 2em;">
              <video src="static/videos/example_1.m4v" controls autoplay muted style="width: 90%; height: auto;"></video>
            </div>
            <div class="publication-video" style="display: flex; flex-direction: column; align-items: center; margin-bottom: 2em;">
              <video src="static/videos/example_2.m4v" controls autoplay muted style="width: 90%; height: auto;"></video>
            </div>
            <div class="publication-video" style="display: flex; flex-direction: column; align-items: center;">
              <video src="static/videos/example_3.m4v" controls autoplay muted style="width: 90%; height: auto;"></video>
            </div>
          </div>
        </div>
      </div>
    </div>

  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">

    <!-- Demo -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Controllable World Models</h2>
        <div class="content has-text-justified">
          <p>
            Recent works have demonstrated that video diffusion models can generate controllable egocentric videos given camera trajectories. Our framework leverages this capability for 3D Spatial Reasoning. The following examples show that multiple different world models are capable enough to support our applications.
          </p>
        </div>
        <h3 class="title is-4 has-text-centered" style="margin-top: 2em;">Stable Virtual Camera</h3>
        <!-- World Model Example Videos -->
        <div style="display: flex; flex-direction: column; align-items: center; gap: 10px;">
          <video src="static/videos/SWMSVC_1.m4v" controls autoplay muted loop style="width: 80%; height: auto;"></video>
          <video src="static/videos/SWMSVC_2.m4v" controls autoplay muted loop style="width: 80%; height: auto;"></video>
        </div>
        
        <h3 class="title is-4 has-text-centered" style="margin-top: 2em;">Search World Model</h3>
        <div style="display: flex; flex-direction: column; align-items: center; gap: 10px;">
          <video src="static/videos/SWMSVC_3.m4v" controls autoplay muted loop style="width: 80%; height: auto;"></video>
          <video src="static/videos/SWMSVC_4.m4v" controls autoplay muted loop style="width: 80%; height: auto;"></video>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- TODO: Change into our bibtex -->
<section class="section" id="BibTeX" style="margin-top: 30mm;">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@misc{yang2025mindjourneytesttimescalingworld,
      title={MindJourney: Test-Time Scaling with World Models for Spatial Reasoning}, 
      author={Yuncong Yang and Jiageng Liu and Zheyuan Zhang and Siyuan Zhou and Reuben Tan and Jianwei Yang and Yilun Du and Chuang Gan},
      year={2025},
      eprint={2507.12508},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2507.12508}, 
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <!-- <div class="content has-text-centered">
      <a class="icon-link"
         href="./static/videos/nerfies_paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/keunhong" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div> -->
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This means you are free to borrow the <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a> of this website,
            we just ask that you link back to this page in the footer.
            Please remember to remove the analytics code included in the header of the website which
            you do not want on your website.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
